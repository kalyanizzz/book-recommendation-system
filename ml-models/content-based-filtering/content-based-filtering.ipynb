{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "849f00d2-90e0-4dd6-909d-32efddd177dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 23:54:53,441 INFO:Initializing Spark Session.\n",
      "2024-12-03 23:54:53,446 INFO:Loading data...\n",
      "2024-12-03 23:55:06,006 INFO:Removing duplicates...                             \n",
      "2024-12-03 23:55:22,175 INFO:Total records after removing duplicates: 166364    \n",
      "2024-12-03 23:55:22,175 INFO:Sampling data...\n",
      "2024-12-03 23:55:38,276 INFO:Total records after sampling: 166364               \n",
      "2024-12-03 23:55:38,284 INFO:Combining text fields...\n",
      "2024-12-03 23:55:38,305 INFO:Preprocessing text...\n",
      "2024-12-03 23:55:38,323 INFO:Tokenizing and removing stop words...\n",
      "2024-12-03 23:55:38,395 INFO:Converting filtered_words to binary vectors using CountVectorizer...\n",
      "2024-12-03 23:56:02,178 INFO:CountVectorizer completed in 23.78 seconds.        \n",
      "2024-12-03 23:56:02,179 INFO:Applying MinHashLSH for approximate similarity join...\n",
      "2024-12-03 23:56:02,220 INFO:MinHashLSH completed in 0.04 seconds.\n",
      "2024-12-03 23:56:02,221 INFO:Performing approximate similarity join...\n",
      "2024-12-03 23:56:02,265 INFO:Filtering out self-matches and selecting top N recommendations...\n",
      "2024-12-03 23:56:02,303 INFO:Saving recommendations to file...\n",
      "2024-12-04 00:17:50,970 INFO:Book recommendation pipeline completed.            \n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat_ws, col, udf, split\n",
    "from pyspark.sql.types import StringType\n",
    "import re\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Initializing Spark Session.\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Connect to HDFS\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://10.0.2.15:9000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Test HDFS access by listing files\n",
    "hdfs_path = \"hdfs:10.0.2.15:9000/user/Group04/Books.csv\"\n",
    "\n",
    "\n",
    "logger.info(\"Loading data...\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if text:\n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove extra spaces\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    else:\n",
    "        text = ''\n",
    "    return text\n",
    "    \n",
    "# Load Data\n",
    "df = spark.read.csv(hdfs_path, header=True, inferSchema=True)\n",
    "df.show(5)\n",
    "\n",
    "logger.info(\"Removing duplicates...\")\n",
    "df = df.dropDuplicates(['Id'])\n",
    "df = df.withColumn('cleaned_title', preprocess_udf(col('Title')))\n",
    "df = df.dropDuplicates(['cleaned_title'])\n",
    "logger.info(f\"Total records after removing duplicates: {df.count()}\")\n",
    "\n",
    "logger.info(\"Sampling data...\")\n",
    "sample_fraction = 1.0  # Adjust as needed\n",
    "df_sample = df.sample(withReplacement=False, fraction=sample_fraction, seed=42)\n",
    "logger.info(f\"Total records after sampling: {df_sample.count()}\")\n",
    "\n",
    "df_sample = df_sample.fillna('')\n",
    "\n",
    "logger.info(\"Combining text fields...\")\n",
    "df_sample = df_sample.withColumn('combined_text', concat_ws(' ',\n",
    "    col('Title'),\n",
    "    col('description'),\n",
    "    col('authors'),\n",
    "    col('categories')\n",
    "))\n",
    "\n",
    "logger.info(\"Preprocessing text...\")\n",
    "\n",
    "preprocess_udf = udf(preprocess_text, StringType())\n",
    "df_sample = df_sample.withColumn('processed_text', preprocess_udf(col('combined_text')))\n",
    "\n",
    "logger.info(\"Tokenizing and removing stop words...\")\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='processed_text', outputCol='words')\n",
    "df_sample = tokenizer.transform(df_sample)\n",
    "\n",
    "remover = StopWordsRemover(inputCol='words', outputCol='filtered_words')\n",
    "df_sample = remover.transform(df_sample)\n",
    "\n",
    "logger.info(\"Converting filtered_words to binary vectors using CountVectorizer...\")\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    inputCol='filtered_words',\n",
    "    outputCol='cv_features',\n",
    "    binary=True,\n",
    "    vocabSize=5000,\n",
    "    minDF=2\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "cv_model = count_vectorizer.fit(df_sample)\n",
    "df_sample = cv_model.transform(df_sample)\n",
    "end_time = time.time()\n",
    "logger.info(f\"CountVectorizer completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "logger.info(\"Applying MinHashLSH for approximate similarity join...\")\n",
    "\n",
    "from pyspark.ml.feature import MinHashLSH\n",
    "\n",
    "mh = MinHashLSH(\n",
    "    inputCol='cv_features',\n",
    "    outputCol='hashes',\n",
    "    numHashTables=3\n",
    ")\n",
    "start_time = time.time()\n",
    "mh_model = mh.fit(df_sample)\n",
    "df_sample = mh_model.transform(df_sample)\n",
    "end_time = time.time()\n",
    "logger.info(f\"MinHashLSH completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "logger.info(\"Performing approximate similarity join...\")\n",
    "\n",
    "threshold = 0.8  # Adjust as needed\n",
    "similarity_df = mh_model.approxSimilarityJoin(\n",
    "    df_sample.select('Id', 'Title', 'cv_features'),\n",
    "    df_sample.select('Id', 'Title', 'cv_features'),\n",
    "    threshold=threshold,\n",
    "    distCol='JaccardDistance'\n",
    ")\n",
    "\n",
    "logger.info(\"Filtering out self-matches and selecting top N recommendations...\")\n",
    "\n",
    "similarity_df = similarity_df.filter(col('datasetA.Id') != col('datasetB.Id'))\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "window_spec = Window.partitionBy('datasetA.Id').orderBy('JaccardDistance')\n",
    "similarity_df = similarity_df.withColumn('rank', F.row_number().over(window_spec))\n",
    "\n",
    "N = 10\n",
    "top_n_df = similarity_df.filter(col('rank') <= N)\n",
    "\n",
    "recommendations_df = top_n_df.groupBy('datasetA.Id', 'datasetA.Title').agg(\n",
    "    F.collect_list('datasetB.Title').alias('Recommended_Titles')\n",
    ").withColumnRenamed('datasetA.Id', 'Id').withColumnRenamed('datasetA.Title', 'Title')\n",
    "\n",
    "# Convert the array of recommended titles to a single string\n",
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "recommendations_df = recommendations_df.withColumn(\n",
    "    'Recommended_Titles',\n",
    "    concat_ws(' | ', 'Recommended_Titles')\n",
    ")\n",
    "\n",
    "# Select the columns to write\n",
    "output_df = recommendations_df.select('Id', 'Title', 'Recommended_Titles')\n",
    "\n",
    "logger.info(\"Saving recommendations to file...\")\n",
    "\n",
    "output_df.write.csv('book_recommendations_sample.csv', header=True, mode='overwrite')\n",
    "\n",
    "logger.info(\"Book recommendation pipeline completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62157f00-8196-4190-90e2-6152e69b3924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LoadAndMergeRecommendations\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the output directory containing part files\n",
    "output_path = \"merged_recommendations.csv\"\n",
    "\n",
    "# Load all part files into a single DataFrame\n",
    "recommendations_df = spark.read.csv(output_path, header=True, inferSchema=True)\n",
    "recommendations_df.coalesce(1).write.option('sep', '###').csv(\"merged_recommendations\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "926f9e13-30c5-4dea-8b16-cd52703e371e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9t/n23lm5dd4lzfbh6qgv30f91c0000gn/T/ipykernel_80789/1278422443.py:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  merged_df = pd.read_csv(merged_file_path, sep='###')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Recommended_Titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64296</th>\n",
       "      <td>B0007HGS0M</td>\n",
       "      <td>Goldfinger ( A Signet Book)</td>\n",
       "      <td>The Trampling of the Lilies | The Sojourner | ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33596</th>\n",
       "      <td>B000N0ZZ3E</td>\n",
       "      <td>Greek Art (Praeger World of Art)</td>\n",
       "      <td>The comedy world of Stan Laurel | Heritage of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33320</th>\n",
       "      <td>B000K7NMEE</td>\n",
       "      <td>Warfare in the Age of Bonaparte</td>\n",
       "      <td>ART OF WARFARE IN THE AGE OF NAPOLEON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50203</th>\n",
       "      <td>B000OXLXIG</td>\n",
       "      <td>All Tucked In... (Harlequin Blaze, 91)</td>\n",
       "      <td>I Thee Bed... (Harlequin Temptation)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49581</th>\n",
       "      <td>B000K5SI5Y</td>\n",
       "      <td>The Christmas Whale</td>\n",
       "      <td>SANTA CLAUS AND HIS ELVES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51125</th>\n",
       "      <td>0393323536</td>\n",
       "      <td>Before &amp; After: Stories from New York</td>\n",
       "      <td>The Famous New York Fundamentalist-Modernist D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23707</th>\n",
       "      <td>B0007DUPYQ</td>\n",
       "      <td>The apple in the dark</td>\n",
       "      <td>Dark Tyrant's Ascension | Dark Carnival | Dark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60360</th>\n",
       "      <td>0789302705</td>\n",
       "      <td>Women's Soccer: The Game and the FIFA World Cup</td>\n",
       "      <td>Micro Fiction: An Anthology of Fifty Really Sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54221</th>\n",
       "      <td>1563923955</td>\n",
       "      <td>Toyota Corolla &amp; Geo/Chevrolet Prizm 1993-2001...</td>\n",
       "      <td>1999 Timing Belts Manual | Manual of ski mount...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37720</th>\n",
       "      <td>0971246807</td>\n",
       "      <td>Healing Client Relationships: A Professional's...</td>\n",
       "      <td>The Loyal Customer: A Lesson From a Cab Driver...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Id                                              Title  \\\n",
       "64296  B0007HGS0M                        Goldfinger ( A Signet Book)   \n",
       "33596  B000N0ZZ3E                   Greek Art (Praeger World of Art)   \n",
       "33320  B000K7NMEE                    Warfare in the Age of Bonaparte   \n",
       "50203  B000OXLXIG             All Tucked In... (Harlequin Blaze, 91)   \n",
       "49581  B000K5SI5Y                                The Christmas Whale   \n",
       "51125  0393323536              Before & After: Stories from New York   \n",
       "23707  B0007DUPYQ                              The apple in the dark   \n",
       "60360  0789302705    Women's Soccer: The Game and the FIFA World Cup   \n",
       "54221  1563923955  Toyota Corolla & Geo/Chevrolet Prizm 1993-2001...   \n",
       "37720  0971246807  Healing Client Relationships: A Professional's...   \n",
       "\n",
       "                                      Recommended_Titles  \n",
       "64296  The Trampling of the Lilies | The Sojourner | ...  \n",
       "33596  The comedy world of Stan Laurel | Heritage of ...  \n",
       "33320              ART OF WARFARE IN THE AGE OF NAPOLEON  \n",
       "50203               I Thee Bed... (Harlequin Temptation)  \n",
       "49581                          SANTA CLAUS AND HIS ELVES  \n",
       "51125  The Famous New York Fundamentalist-Modernist D...  \n",
       "23707  Dark Tyrant's Ascension | Dark Carnival | Dark...  \n",
       "60360  Micro Fiction: An Anthology of Fifty Really Sh...  \n",
       "54221  1999 Timing Belts Manual | Manual of ski mount...  \n",
       "37720  The Loyal Customer: A Lesson From a Cab Driver...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the merged CSV with tab delimiter\n",
    "merged_file_path = \"merged_recommendations/part-00000-bf156df3-4792-41ad-9235-d683b7fbc646-c000.csv\"  # Adjust as needed\n",
    "merged_df = pd.read_csv(merged_file_path, sep='###')\n",
    "\n",
    "# Show random row\n",
    "merged_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cd235f-9920-4808-8e18-3799483af02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PySpark)",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
